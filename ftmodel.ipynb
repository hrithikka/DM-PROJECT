{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mounting Google Drive: which is used to save the fine-tuned model and tokenizer."
      ],
      "metadata": {
        "id": "U9evCmAklsGc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ8yacmT2Xsy"
      },
      "outputs": [],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mN7Ap0OFhiXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/M3/"
      ],
      "metadata": {
        "id": "N6inW38V23hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Required Packages"
      ],
      "metadata": {
        "id": "qIFAsMnCmGL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "2a9s_p5r27sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "NJiyA9m73HJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "JpTLMXC13LzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "model_name = \"distilbert-base-uncased\""
      ],
      "metadata": {
        "id": "AFMvbfwt3RNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Dataset: loads the custom dataset using the load_dataset function from the datasets package."
      ],
      "metadata": {
        "id": "LdMdZJLimZK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dict = load_dataset('HUPD/hupd',\n",
        "                            name='sample',\n",
        "                            data_files=\"https://huggingface.co/datasets/HUPD/hupd/blob/main/hupd_metadata_2022-02-22.feather\",\n",
        "                            icpr_label=None,\n",
        "                            train_filing_start_date='2016-01-01',\n",
        "                            train_filing_end_date='2016-01-21',\n",
        "                            val_filing_start_date='2016-01-22',\n",
        "                            val_filing_end_date='2016-01-31')"
      ],
      "metadata": {
        "id": "fseQ6qBj3VDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F4cSpsajnbwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loding the training and testing datasets from the HUPD/hupd dataset and converts them into Pandas dataframes."
      ],
      "metadata": {
        "id": "6GjFcM1ZoEBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = dataset_dict['train'].to_pandas()\n",
        "test_df = dataset_dict['validation'].to_pandas()"
      ],
      "metadata": {
        "id": "rXRzMngE3mhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the training data by concatenating the abstract and claims text from the training set, and assigning the label 0 for the abstract and label 1 for the claims."
      ],
      "metadata": {
        "id": "sChIZ5uJoHPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = list(train_df['abstract']) + list(train_df['claims'])\n",
        "train_labels = [0] * len(train_df) + [1] * len(train_df)"
      ],
      "metadata": {
        "id": "H60sb3lf3q0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating lists of training and testing texts by concatenating the abstract and claims columns of the respective dataframes, and creating labels for the texts based on their origin (0 for training and 1 for testing)."
      ],
      "metadata": {
        "id": "FGy9DBezowQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = list(test_df['abstract']) + list(test_df['claims'])\n",
        "test_labels = [0] * len(test_df) + [1] * len(test_df)"
      ],
      "metadata": {
        "id": "CXNm4YjU3t9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenizer instance for the specified pre-trained DistilBERT model"
      ],
      "metadata": {
        "id": "ukK1Ig8GozgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "bUmr3veq3wml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the tokenizer object to encode the train_texts and test_texts into numerical encodings suitable for models."
      ],
      "metadata": {
        "id": "Cqq1_UHkpBD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "6jyLw5Hx3zXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a class definition for creating a custom PyTorch dataset named FTDataset. The dataset takes in two arguments, encodings and labels, which are the encoded texts and corresponding labels respectively."
      ],
      "metadata": {
        "id": "B31UK4EZpjdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FTDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "CUM6egDu35Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiating FTDataset objects for the train and test sets using the encoded texts (train_encodings and test_encodings) and labels (train_labels and test_labels). This step creates PyTorch Dataset objects that can be fed into the DataLoader later for training and evaluation."
      ],
      "metadata": {
        "id": "FU503bxlqCXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = FTDataset(train_encodings, train_labels)\n",
        "test_dataset = FTDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "W1hL_73839Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing a pre-trained transformer model for sequence classification using the AutoModelForSequenceClassification class from the Hugging Face Transformers library"
      ],
      "metadata": {
        "id": "bmezjo5Fqgf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        ")"
      ],
      "metadata": {
        "id": "WUA1eyBc3_7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing the training arguments for the model, including the number of training epochs, batch size, and learning rate, among other hyperparameters."
      ],
      "metadata": {
        "id": "Zg_GE71yqyIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',            \n",
        "    num_train_epochs=2,                \n",
        "    per_device_train_batch_size=32,    \n",
        "    per_device_eval_batch_size=64,     \n",
        "    warmup_steps=500,                  \n",
        "    learning_rate=5e-5,                \n",
        "    weight_decay=0.01,                 \n",
        "    logging_dir='./logs',              \n",
        "    logging_steps=10,\n",
        ")"
      ],
      "metadata": {
        "id": "XcT7eJM84IyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer object which is responsible for training the model using the specified training and evaluation datasets, along with the given hyperparameters and settings."
      ],
      "metadata": {
        "id": "jM0Bpd_Uq45B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                       \n",
        "    args=training_args,                \n",
        "    train_dataset=train_dataset,       \n",
        "    eval_dataset=test_dataset           \n",
        ")"
      ],
      "metadata": {
        "id": "xOZ4NpWw4O-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "trains the specified model on the train_dataset according to the args specified in training_args."
      ],
      "metadata": {
        "id": "NyGQcCnUrMpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "EFsjkUHT4TzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluates the trained model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0o5yy7Qzrd2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()"
      ],
      "metadata": {
        "id": "fkyiDcp97IFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the fine-tuned DistilBERT model, along with its configuration and vocabulary, to the specified directory "
      ],
      "metadata": {
        "id": "Hv323Gnlrvbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./results/saved_model\")"
      ],
      "metadata": {
        "id": "Bnoc0ezh7hBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving the trained tokenizer"
      ],
      "metadata": {
        "id": "V_nnzHDYsLUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"./results/saved_model\")\n",
        "model.save_pretrained(\"./results/saved_model\")"
      ],
      "metadata": {
        "id": "bKiiWk5-Gr7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}